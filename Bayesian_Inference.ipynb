{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Our Misssion ##\n",
    "Spam detection is one of the major applications of Machine Learning in the interwebs today. Pretty much all of the major email service providers have spam detection systems built in and automatically classify such mail as 'Junk Mail'.\n",
    "In this mission we will be using the Naive Bayes algorithm to create a model that can classify [dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) SMS messages as spam or not spam, based on the training we give to the model. It is important to have some level of intuition as to what aspammy text message might look like. Usually they have words like 'free', 'win', 'winner', 'cash', 'prize' and the like in them as these texts are designed to catch your eye and in some sense tempt you to open them. Also, spam messages tend to have words written in all capitals and also tend to use a lot of exclamation marks. To the recipient, it is usually pretty straightforward to identify a spam text and our objective here is to train a model to dothat for us!\n",
    "Being able to identify spam messages is a binary classification problem as messages are classified as either 'Spam' or 'Not Spam' and nothing else. Also,this is a supervised learning problem, as we will be feeding a labelled dataset into the model, that it can learn from, to make future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 0: Introduction to the Naive Bayes Theorem ###\n",
    "\n",
    "Bayes theorem is one of the earliest probabilistic inference algorithms developed by Reverend Bayes(Intresting Fact -read online what actually he was trying to do ..) and still performs extremely well for certain use cases.\n",
    "\n",
    "It's best to uderstand this theorem using an example. Let's say you are member of the Secret Setvice and you have been deployed to protect the Democratic presendential nominee during one of his/her campaign speeches.Being a public event that is open to all, your job is not easy and you have to be on constant lookout for threats. So one place to start is to put a certain threat-factor for each person. So based on the features of an individual, like the age,sex and other smaller factors like is the person carrying a bag ?, does the person look nervous? etc.. you can make a judgement call as to if that perison is viable threat.\n",
    "If an individual ticks all the boxes up to a level where it crosses a threshold of doubt in your mind, you can take action and remove that person from the vicinity. The Bayes theorem works in the same way as we are computing the probability of an event(a person being threat) based on the probabilities of certain related events(age,sex,presence of bag or not, nervousness etc. of the person).\n",
    "\n",
    "One thing to consider is the independence of these features amongst each other.For example if a child looks nervous at the event then the likelihood of that person being a threat is not much as say it was a grown man who was nervous.To break this down own a bit further, here there are two features we are consideirng, age AND nervousness. Say we look at these features individually, we could design a model that flags ALL persons that are nervous as potential threats. However, it is likely that we will have lot of false positives as there is a string chance that minors present at the event will be nervous.Hence by the consideration the age of the person along with the 'nervousness' feature would definitely get a more accurate results.\n",
    "\n",
    "This is the 'NAIVE' bit of theorem where it considers each feature to be independent of each other which may not always be the case and hence that can effect the final judgement.\n",
    "\n",
    "In short, the Bayes theorem calculates the probability of a certain event happening(in our case, a message being  spam) based on the joint probabilistic distributions of certain other events(in our case, a message being classified as spam). We will dive into the workings of the Bayes theorem later in the mission, but first, let us understand the data we are going to work with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.1 Understanding our Data\n",
    "\n",
    "We will be using a [dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) from the UCI Machine Learning repository which has a very good collection of datasets for experimental research purposes. The direct data link is [here](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/).\n",
    " \n",
    " ** Here's a preview of the data: ** \n",
    "\n",
    "<img src=\"images/spamdata.png\" height=\"1242\" width=\"1242\">\n",
    "\n",
    "The columns in the data set are currently not named and as you can see, there are 2 columns. \n",
    "\n",
    "The first column takes two values, 'ham' which signifies that the message is not spam, and 'spam' which signifies that the message is spam. \n",
    "\n",
    "The second column is the text content of the SMS message that is being classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">** Instructions: **\n",
    "* Import the dateset using pandas dataframe using the read_table method.Because this is a tab seperated dataset we will be usingn '\\t' as the value for 'sep' argument which \n",
    "specifies this format.\n",
    "* Also, rename the columns by specific a list ['label','sms_message'] to the 'names' argument of read_table().\n",
    "* Print the first five values of the dataframe with the new column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Solution\n",
    "'''\n",
    "\n",
    "\n",
    "import  pandas as pd\n",
    "\n",
    "# Dataset from https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
    "\n",
    "df = pd.read_table('smsspamcollection/SMSSpamCollection', sep='\\t', header =None, names =['label','sms_message'])\n",
    "\n",
    "#Ouput printing First 5 items\n",
    " \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1.2: Data Preprocessing ###\n",
    "\n",
    "Now that we have basic understanding of what our dataset looks like, lets convert our labels to binary varibales,0 to represent 'ham'(i:1 not spam) and \n",
    "1  to represent 'spam' for ease of computation\n",
    "    \n",
    "You need to convert beacuse of how scikit-learn handles input . We will be to make predictions with strings but later to calculate precison and recall scores scikit-learn will convert \n",
    "these string to some unknown float values which can cause issues later. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">**Instructions: **\n",
    "* Convert the values in 'label' column to numerica values using map method as follows:\n",
    "{'ham':0,'spam':1} This maps the 'ham' value to 0 and the 'spam' value to 1\n",
    "* Also, to get an idea of the size of the dataset we are dealing with, print out numbers of rows and columns using 'shap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Solution\n",
    "'''\n",
    "\n",
    "df['label'] = df.label.map({'ham':0,'spam':1})\n",
    "print(df.shape)\n",
    "df.head() \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Srep 2.1: Bag of words ###\n",
    "What we have in our data set is a large colleciton of data(5,572 rows of data).Most ML algorithms rely on numerical data to be fed into them as input, and emails/sms messages\n",
    "are usually text heavy.  \n",
    "Here udacity introduces the BAG of Words(Bow) concept which is a term used to specify the problems which have a 'bag of words' or a colleciton of text data that needs to be\n",
    "worked with. The basic idea of BOW is to take a peice of tect and count the frequency of the words in a text. It is importatn to note that BoW concept treats each word \n",
    "individually and in order in which the words occur does not matter\n",
    "\n",
    "Using a process ehich we will go through now, we can covert a collection of documents to a matrix, with each document being a row  and each word(token) being the column, and\n",
    "the corresponding(roq,column) values being the frequency of occurance of each word or token in that document. \n",
    "\n",
    "Lets say we have 4 documents as follows:\n",
    "\n",
    "`['Hello, how are you!',\n",
    "'Win money, win from home.',\n",
    "'Call me now',\n",
    "'Hello, Call you tomorrow?']`------\n",
    "\n",
    "Our objective here is to convert this set of text to a frequency distribution matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting the Data to training and testing parts ###\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['sms_message'],df['label'],random_state=1)\n",
    "\n",
    "print('Number of rows in total set: {}'.format(df.shape[0]))\n",
    "print('Number of rows in training set:{}'.format(X_train.shape[0]))\n",
    "print('Number of rows for testing set:{}'.format(X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Counter Vector here ###\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a instance of CountVectorizer\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "# Fit the training data and then return the matrix\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "\n",
    "# Transform tecsting data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()\n",
    "testing_data = count_vector.transform(X_test)\n",
    "\n",
    "print(training_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Naive_Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now predict using or Model and store it in predictions variable\n",
    "\n",
    "predictions = naive_bayes.predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3: Evaluating our model ###\n",
    "\n",
    "As now we are done training our Model and we have predictions too !!! Yippee!! FInally!! Wait Dude Last Step !!!\n",
    "\n",
    " ### 3.1 LAST STEP --> Evaluate how well our model is doing! Remember Videos?? Lets Recap a little. As it is a regression\n",
    "Problem and toolkit  for measuring model is following:\n",
    "\n",
    "** Accuracy ** measures how often the classifier makes the correct prediction. Itâ€™s the ratio of the number of correct predictions to the total number of predictions (the number of test data points).\n",
    "\n",
    "** Precision ** tells us what proportion of messages we classified as spam, actually were spam.\n",
    "It is a ratio of true positives(words classified as spam, and which are actually spam) to all positives(all words classified as spam, irrespective of whether that was the correct classification), in other words it is the ratio of\n",
    "\n",
    "`[True Positives/(True Positives + False Positives)]`\n",
    "\n",
    "** Recall(sensitivity)** tells us what proportion of messages that actually were spam were classified by us as spam.\n",
    "It is a ratio of true positives(words classified as spam, and which are actually spam) to all the words that were actually spam, in other words it is the ratio of\n",
    "\n",
    "`[True Positives/(True Positives + False Negatives)]`\n",
    "\n",
    "For classification problems that are skewed in their classification distributions like in our case, for example if we had a 100 text messages and only 2 were spam and the rest 98 weren't, accuracy by itself is not a very good metric. We could classify 90 messages as not spam(including the 2 that were spam but we classify them as not spam, hence they would be false negatives) and 10 as spam(all 10 false positives) and still get a reasonably good accuracy score. For such cases, precision and recall come in very handy. These two metrics can be combined to get the F1 score, which is weighted average of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('Accuracy Score', format(accuracy_score(y_test,predictions)))\n",
    "print('Precision Score', format(precision_score(y_test,predictions)))\n",
    "print('Recall Score', format(recall_score(y_test,predictions)))\n",
    "print('F1 Score', format(f1_score(y_test,predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ### Results ###\n",
    " \n",
    " Posting above results\n",
    " \n",
    "('Accuracy Score', '0.988513998564')\n",
    "('Precision Score', '0.972067039106')\n",
    "('Recall Score', '0.940540540541')\n",
    "('F1 Score', '0.956043956044')\n",
    "\n",
    "\n",
    "As  --- >The F1 score can be interpreted as a weighted average of the precision and recall,\n",
    " where an F1 score reaches its best value at 1 and worst score at 0. \n",
    " The relative contribution of precision and recall to the F1 score are equal.\n",
    " \n",
    " "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
